{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f11b4737-796b-4734-86c3-95721afdb0ce",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8379bb3e-c238-434b-84b1-5affacaaeffd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor, ResNetForImageClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "afb67ae7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd38a8c8-b505-4005-a7c7-171f264bc141",
   "metadata": {},
   "source": [
    "# Import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "edc38df0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = ResNetForImageClassification.from_pretrained('./resnet18')\n",
    "model.eval()\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"./resnet18\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778091b5-07dd-4456-98f0-99d91380864e",
   "metadata": {},
   "source": [
    "# Import test images and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "630de2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all image files in current directory\n",
    "images = []\n",
    "for file in os.listdir('./images'):\n",
    "    if(\".jpg\" in file):\n",
    "        images.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "03ebf3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for i in images:\n",
    "    image = Image.open(os.path.join('.','images',i))\n",
    "    inputs = feature_extractor(image, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    predicted_label = logits.argmax(-1).item()\n",
    "    predictions.append(model.config.id2label[predicted_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "445731ea-812a-4c80-b43b-3ab8d56e2c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aeroplane.jpg', 'cat.jpg', 'flower.jpg', 'pigeon.jpg', 'football.jpg']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "81df07f6-5dd5-4e3b-a549-f1c58e306ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airliner',\n",
       " 'tabby, tabby cat',\n",
       " 'rapeseed',\n",
       " 'European gallinule, Porphyrio porphyrio',\n",
       " 'soccer ball']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7116a7-fd3f-4664-8f73-31e89080321d",
   "metadata": {},
   "source": [
    "# Export model to ONNX format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4f45282e-a39a-432b-b799-65d688a6aacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy input, required to establish approximate computational graph\n",
    "dummy_input = torch.randn(1, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f2415f71-9be3-47e5-9770-5a6279f6c113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s4m13337/Developer/torch2onnx/lib/python3.10/site-packages/transformers/models/resnet/modeling_resnet.py:95: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if num_channels != self.num_channels:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(model, dummy_input, \"resnet18.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e60c9907-0a17-43ea-b89d-89b192255894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[[-0.6452, -0.6965, -0.7650,  ...,  0.2453,  0.2282,  0.2111],\n",
       "          [-0.6452, -0.7137, -0.7822,  ...,  0.2111,  0.1939,  0.1768],\n",
       "          [-0.6281, -0.7308, -0.7822,  ...,  0.1254,  0.1426,  0.1254],\n",
       "          ...,\n",
       "          [-1.0904, -1.3815, -1.5014,  ..., -0.7993, -0.8678, -0.9020],\n",
       "          [-1.1075, -1.2959, -1.2274,  ..., -0.8335, -1.0733, -1.0390],\n",
       "          [-1.3815, -1.1589, -0.8678,  ..., -1.0219, -1.0733, -1.0219]],\n",
       "\n",
       "         [[ 0.3627,  0.3452,  0.3102,  ...,  0.7829,  0.7654,  0.7479],\n",
       "          [ 0.3102,  0.2927,  0.2402,  ...,  0.7304,  0.7129,  0.7129],\n",
       "          [ 0.2927,  0.2402,  0.2052,  ...,  0.6779,  0.6779,  0.6779],\n",
       "          ...,\n",
       "          [-0.6352, -0.8452, -0.9503,  ..., -0.0224, -0.1625, -0.3025],\n",
       "          [-0.5826, -0.6527, -0.4951,  ..., -0.1800, -0.5126, -0.5301],\n",
       "          [-0.7752, -0.3025,  0.0301,  ..., -0.5301, -0.6527, -0.5651]],\n",
       "\n",
       "         [[-1.2816, -1.2641, -1.2816,  ..., -0.7238, -0.7064, -0.7413],\n",
       "          [-1.2816, -1.2467, -1.2816,  ..., -0.8284, -0.8110, -0.8110],\n",
       "          [-1.2816, -1.2990, -1.3164,  ..., -0.9156, -0.8807, -0.8807],\n",
       "          ...,\n",
       "          [-1.2816, -1.4384, -1.5604,  ..., -0.7761, -0.8633, -1.0027],\n",
       "          [-1.3687, -1.3861, -1.4210,  ..., -0.8807, -1.1421, -1.2293],\n",
       "          [-1.5604, -1.4210, -1.1247,  ..., -1.1596, -1.2119, -1.2293]]]])}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e02688f-38cc-424e-a906-54266c102fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
